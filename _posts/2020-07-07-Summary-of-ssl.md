---
layout: article
title: Summary：半监督学习
date: 2020-07-07 00:10:00 +0800
tags: [Summary, Deep Learning]
categories: blog
pageview: true
key: Summary-of-SSL
---

------

## 参考

- https://www.jiqizhixin.com/articles/2018-12-24-26
- http://www.fenghz.xyz/semi-supervised-learning/
- https://zhuanlan.zhihu.com/p/33196506
- https://zhuanlan.zhihu.com/p/39367595
- https://www.zhihu.com/question/265479171
- https://www.jianshu.com/p/e908c3595fc0



## 什么是半监督学习



## 主动学习(Active Learning)

### 背景

深度学习的训练依赖大量标注数据，从训练成本和时间两方面考虑，大规模数据标注成本高、训练时间长。主动学习等方法可以降低标注成本，减小训练时间。

### 原理

主动学习通过设计合理的查询函数，从未标注的数据中查询**最有用的**样本，并交由专家进行标注，加入训练集，重训练模型，反复迭代，提高模型的精确度。

**算法流程**：

1. （**学习**）：利用已有的标注样本训练分类器；
2. （**查询**）：将未标注的样本输入分类器，得到预测值（或通过查询函数$$Q$$查询样本）；
3. （**选择**）：根据查询结果从未标注样本中筛选出**信息量较大**的样本，对这批样本进行人工标注；
4. （**学习**）：用新标注的样本重训练原始分类器，重复2~3，直到
   - 当前分类器对选出来的数据正确分类；
   - 人工无法对筛选的样本进行分类标注；

### 设计准则

上述算法流程中我们需要注意几点：

- 查询函数$$Q$$用于查询一个或一批最有用的样本，这里包含一些概念，即度量标准对查询函数的设计提出了要求，包括不确定性（uncertainty）、差异性（diversity）、代表性、方差和错误率：

  - **不确定性认为最重要的未标注数据就是最接近当前分类边界的数据。**可以借助信息熵的概念来进行理解。我们知道信息熵是衡量信息量的概念，也是衡量不确定性的概念。信息熵越大，就代表不确定性越大，包含的信息量也就越丰富。事实上，有些基于不确定性的主动学习查询函数就是使用了信息熵来设计的，比如熵值装袋查询（Entropy query-by-bagging）。所以，**不确定性策略就是要想方设法地找出不确定性高的样本，因为这些样本所包含的丰富信息量，对我们训练模型来说就是有用的。**

    例如，在训练svm时，我们挑选距离分类超平面最近的未标注样本来进行查询，或者说对预测置信度接近0.5的样本进行查询。

  - **差异性认为在多个不同基准分类器中具有最大预测差异的未标注数据更为重要。**查询函数每次迭代的查询一个或一批样本，我们希望所查询的样本能提供更全面的信息，也就是每个样本提供的信息尽可能不冗余，样本之间保持一定的差异性。如果是查询单个样本，我们可以选择信息量最大的样本加入训练集；如果是查询一批样本，应该尽可能保证样本差异性。
  - **代表性认为可以表示一组新实例（例如一个聚类）的未标注数据更为重要。**

### 分类

根据输入数据的方式，主动学习可以分为：

- **基于流的主动学习**，它将未标记的数据一次性全部呈现给一个预测模型，该模型将预测结果（实例的概率值），根据某些评价指标（比如margin）计算评估实例的价值，随后应用主动学习决定是否应该花费一些预算来收集此数据的**类标签**，以进行后续的训练；
- **基于池的主动学习**，这个通常是离线、反复的过程。这里向主动学习系统提供了大量未标记的数据，在此过程的每个迭代周期，主动学习系统都会选择一个或者多个未标记数据进行标记并用于随后的模型训练，直到预算用尽或者满足某些停止条件为止。此时，如果预测性能足够，就可以将模型合并到最终系统中，该最终系统为模型提供未标记的数据并进行预测。



根据数据选择的角度，可以分为具有渐进关系的两类：

- 一是仅基于独立同分布（IID）数据的不确定性进行主动学习，其中选择标准仅取决于针对每个数据自身信息计算的不确定性值；
- 二是通过进一步考虑实例相关性来进行主动学习，基于数据相关性的不确定性度量标准，利用一些相似性度量来区分数据之间的差异。



