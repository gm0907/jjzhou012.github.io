---
layout: article
title: 图的对抗攻击： Adversarial Examples on Graph Data - Deep Insights into Attack and Defense
date: 2019-12-23 00:18:00 +0800
tags: [Adversarial attack, Graph]
categories: blog
pageview: true
key: Adversarial-Attack-on-Graph-1 
---

------

论文链接：[https://www.ijcai.org/proceedings/2019/0669.pdf](https://www.ijcai.org/proceedings/2019/0669.pdf)

吐槽一下，这篇论文有很多错误。。。

## Introduction

图数据深度学习模型，如图卷积网络(GCN)，在处理图数据时表现出了良好的性能。与其他类型的深度学习模型类似，图的深度学习模型也经常受到攻击。与非图数据相比，图数据的离散特征、图连接以及对不可察觉扰动的不同定义为图数据的对抗攻击和防御带来了独特的挑战和机会。

本文提出了攻击和防御技术。对于离散性问题，作者证明了通过引入积分梯度可以很容易地解决，积分梯度可以准确地反映出每个特征或边的影响，同时还能从并行计算中获益。对于防御，他们观察到有目标攻击的对抗样本不同于统计性的图。在此基础上，他们提出了一种防御方法，即检查图并恢复潜在的对抗扰动。

提出了目前在图结构数据上应用对抗攻击的挑战——————**输入的离散性问题**。图节点的特征是离散的，图的边也是离散的。

在攻击方面，作者表示，尽管存在离散输入问题，梯度仍然可以用积分梯度来精确逼近。通过将部分梯度与输入特征(从参考输入到实际输入)进行积分，得到近似的Shapley值。与迭代方法相比，集成梯度大大提高了节点和边缘选择的效率。

在防御方面，作者表示，图模型(如GCN)存在缺陷的一个关键原因是这些模型本质上是根据图结构来聚集特征的。它们在对目标节点进行预测时，严重依赖于最近邻的信息。他们研究了现有攻击技术所造成的扰动，发现添加连接不同特征节点的边在所有攻击方法中起着关键作用。

本文证明了对图的邻接矩阵进行简单的预处理就可以识别修改的边。对于有词包(BOW)特征的节点，Jaccard指标在度量节点之间的相似性时是有效的。通过移除连接非常不同的节点的边，他们能够在不降低GCN模型准确性的情况下防御有针对性的对抗攻击。



## Preliminaries

### Graph Convolutional Network

这个不再赘述，见[GCN理解](https://jjzhou012.github.io/blog/2019/12/07/gcn-understanding.html)。

不过对符号还是得提一下。

给定一个图$G=(A,X)$，$A\in [0,1]^{N\times N}$为邻接矩阵，$X\in[0,1]^{D}$为$D$维二值化特征矩阵。

考虑半监督节点分类任务，部分节点$V_{\mathcal{L}} \subseteq V$被标注$$\mathcal{C}=\{1,2,\cdots,c_K\}$$，需要对图中剩余节点进行分类标注。

GCN模型很好的应用于半监督节点分类，该模型通过以下方式聚合邻域特征：


$$
H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)
$$


在实际中通常使用两层的GCN模型，可以表示为：


$$
Z=f(X, A)=\operatorname{softmax}\left(\hat{A} \sigma\left(\hat{A} X W^{(0)}\right) W^{(1)}\right)
$$


其中$\hat{A}=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$，$W$是需要训练的权重矩阵，不在赘述。



### Gradients Based Adversarial Attacks

梯度常用于攻击深度模型。不仅可以使用损失函数的梯度，也能使用模型输出的梯度。

在图像领域的两者常用的对抗攻击方法：

- Fast Gradient Sign Method (FGSM) attack

  FGSM通过对图像中每个像素的损失函数的梯度的符号方向进行梯度更新来生成对抗样本。扰动可以被表示为：

  $$
  \eta=\epsilon \operatorname{sign}\left(\nabla J_{\theta}(x, l)\right)
  $$

	其中$\epsilon$表示扰动大小，$J$为损失函数，生成的样本为$x'=x+\eta$。

- Jacobian-based Saliency Map Approach (JSMA) attack

  通过利用DNN模型的前向导数，可以发现对抗性扰动，这种扰动迫使模型将测试点错误地划分为特定的目标类。给定一个前馈神经网络$\mathbf{F}$和样本$\mathbf{X}$，Jacobian可以通过以下方式计算：
  
  $$
  \nabla F(X)=\frac{\partial F(X)}{\partial X}=\left[\frac{\partial F_{j}(X)}{\partial x_{i}}\right]_{i \in 1 \ldots M, j \in 1 \ldots N}
  $$
  
  其中模型输出的维度为$M$，数据输入维度为$N$。为了实现目标攻击将样本错误分类为$t$，我们希望$F_t(X)$增大，当$j \neq t$时， $F_j(X)$减小。通过利用对抗显著性映射实现的攻击如下：
  
  
  $$
  S(X, t)[i]=\left\{\begin{array}{l}
  {0, \text { if } \frac{\partial F_{i}(X)}{\partial F_{i}}<0 \text { or } \Sigma_{j \neq t} \frac{\partial F_{i}(X)}{\partial X_{i}}>0} \\
  {\frac{\partial F_{t}(X)}{\partial X_{i}}\left|\Sigma_{j \neq t} \frac{\partial F_{j}(X)}{\partial X_{i}}\right|, \text { otherwise }}
  \end{array}\right\}
  $$
  
  从一个普通的样本开始，攻击者遵循显著性映射，以非常小的数量迭代地扰动样本，直到翻转预测的标签。对于无目标的攻击，攻击者试图最小化可能类标的预测分数。



### Defense for Adversarial Examples

少量的防御工作已经提出了。诸如局部平滑、图像压缩等，这些预处理工作是基于观察到自然图像的相邻像素通常是相似的假设。其他的防御方法还有对抗训练。



## Integrated Gradients Guided Attack

FGSM和JSMA在图像领域的成果得益于图像数据的连续性，但是应用在图结构数据上没有取得很好的成效，一些工作使用贪婪算法或者强化学习来克服其中的问题，但是代价太昂贵。

图中的节点特性通常是词包类型的特性，即1或0。图中未加权的边也经常被用来表示特定关系的存在，因此在邻接矩阵中只有1或0。当攻击模型时，对抗扰动被限制为要么从1变到0，要么反之。在图模型中应用普通FGSM和JSMA的主要问题是不准确的梯度。

对于一个目标节点$t$，对于FGSM攻击来说，通过以下方式来测量所有节点对损失函数的特征重要性：


$$
\nabla J_{W^{(1)}, W^{(2)}}(t)=\frac{\partial J_{W^{(1)}, W^{(2)}}(t)}{\partial X}
$$


其中，$X$为特征矩阵，每一行用于描述一个节点的特征。对于节点$n$的具体特征$i$，$\partial {J_{W^{(1)}, W^{(2)}}}_{in} $ 越大表明将特征$i$扰动为1有助于错误分类目标节点。

但是这里存在两个问题：

- 特征$i$的值可能已经为1，无法进行扰动；
- 即使特征值为0，由于GCN模型可能不会对这个特征值学习0到1之间的局部线性函数，所以这种扰动的结果是不可预测的；

这些问题在JSMA中也同样存在，因为该模型的雅可比矩阵共享了损失函数梯度的所有约束。换句话说，普通梯度算法存在局部梯度问题。以一个简单的ReLU网络$f(x) = ReLU(x-0.5)$为例，当$x$从0增加到1时，函数值增加0.5。然而，在$x = 0$处计算梯度得到0，这并不能准确地捕获模型行为。

为了解决这些问题，论文提出了基于积分梯度的方法，不直接使用普通导数进行攻击。积分梯度最初是由Sundararajan等人提出的，目的是为深度神经网络，特别是图像卷积神经网络中的特征属性提供灵敏度和实现不变性。



### Methods

积分梯度定义如下：

对给定模型$\mathbf{F} : R^n \rightarrow [0,1]$，输入为$x\in R^n$，基准 输入为$x'$。考虑从$x'$到输入$x$的直线路径，通过对路径上所有点的梯度进行累加得到积分梯度。对于输入$x$的第$i$个特征，积分梯度(IG)按如下表示：


$$
I G_{i}(F(x))::=\left(x_{i}-x_{i}^{\prime}\right) \times \int_{\alpha=0}^{1} \frac{\partial F\left(x^{\prime}+\alpha \times \left(x-x^{\prime}\right)\right)}{\partial x_{i}} d \alpha
$$


对于GCN模型，论文提出了一个攻击框架。给定邻接矩阵$A$，特征矩阵$X$，目标节点$t$，计算函数$F_{W^{(1)}, W(2)}(A, X, t)$关于$I$的积分梯度，其中$I$是攻击的输入，$I=A$为边攻击，$I=X$为特征攻击。

- 当$F$为GCN模型的损失函数，称此攻击为IG-FGSM​。
- 当$F$为GCN模型的预测输出，称此攻击为IG-JSMA。

对于不同类型的攻击：

- 对目标攻击而言，优化目标是最大化$F$的值。

  对于图中值为1的特征或者存在的边，选择那些负IG分数最低的对象，

  - 对于特征，将其值设置为0；
  - 对于边，将其删除；

- 对于无目标攻击，需要最小化期望类标的函数值。

  对于图中值为1的特征或者存在的边，选择那些正IG分数最高的对象，

  - 对特征而言，将其值设置为0；
  - 对边进行删除；

对于基准输入，用all-zero或者all-one的特征/邻接矩阵表示$1\rightarrow 0$或者$0 \rightarrow 1$的扰动。也就是说，当删边或者将特征从1变为0时，需要将邻接/特征矩阵改为all-zero；反之当增边或者将特征从0变为1时，需要将邻接/特征矩阵改为all-one。

为了保证梯度的一致性和攻击的可操作性，$I G(F(X, A, t))[i, j]$（边攻击）可以近似如下：

$$
\left\{\begin{array}{l}
{\frac{A_{i j}}{m} \times \sum_{k=1}^{m} \frac{\partial F\left(\frac{k}{m} \times\left(A_{i j}-0\right)\right)}{\partial A_{i j}}, \text { removing edges }} \\
{\frac{1-A_{i j}}{m} \times \sum_{k=1}^{m} \frac{\partial F\left(A_{i j}+\frac{k}{m} \times\left(1-A_{i j}\right)\right)}{\partial A_{i j}}, \text { adding edges }}
\end{array}\right.
$$

该公式其实是上述公式（7）的离散形式。



积分梯度随后被用来衡量改变图$G$中特定特征或边的重要性。注意，只计算边不存在的情况下添加边的重要性，或者只计算在特征为0的情况下将其更改为1的重要性，反之亦然。因此，对于重要的特性或边，只需将二进制值翻转即可。





## Defense for Adversarial Graph Attacks

为了防御针对GCN模型的目标攻击，假设GCN模型在攻击图上进行训练。在这种情况下，一种可行的防御方法是使邻接矩阵可训练。通过在训练过程中学习选择的边的权值，有可能改变图的结构，使使用普通GCN对原始图进行攻击变得无效。

论文通过使边权值在GCN模型中可训练来验证这一思想。对于CORA-ML数据集，选择一个能被训练过的模型正确分类的节点。然后使用nettack构造对抗样本。在没有任何防御的情况下，攻击后目标节点以0.998的置信度被误分类。然后使用论文的防御策略。从对抗样本开始，我们不修改损失函数对GCN模型进行训练，除了添加选定的可训练的边权值。有趣的是，使用这种简单的防御方法，攻击后对目标节点的分类准确率为0.912。

为什么防御策略生效呢？观察到：

- 攻击时，对边进行扰动，比修改特征更有效。可能的解释是，修改边可以对边包含的所有特征进行扰动，而修改特征造成的影响相对较少；
- 攻击更倾向于加边而不是删边；
- 度越大的节点越难被攻击，即不论在干净的样本还是对抗样本中，度越大的节点越容易被分类正确；
- 攻击时，倾向于将目标节点连接到差异较大（具有不同特征和标签）的节点，这是实现攻击的最有效的方法。

>  论文使用jaccard相似性指标来衡量节点的相似性，jaccard通过计算两个节点特征的重合度来衡量他们之间的相似度：
>  
> $$
> J_{u, v}=\frac{\sum_{i=1}^{n} u_{i} v_{i}}{\sum_{i=1}^{n} u_{i}+v_{i}-u_{i} v_{i}}
> $$
> 
考虑到相似性指标在不同数据集之间可能存在较大的差异，特别是对其他类型的特征，可以考虑使用不同类型的相似性指标。


论文在CORA-ML数据集上训练了一个两层的GCN，并研究了被正确分类的高概率节点(即≥0.8)。对于这些节点，下图1显示了FGSM攻击前后的节点对的jaccard相似性指标的直方图。可以很明显的看出来，对抗攻击增加了与目标节点相似度很低的邻居节点的数量。

![00e0c081c194e8934f7701426bcbe56.png](http://ww1.sinaimg.cn/mw690/005NduT8ly1ga6pf9yv5lj30ge09274z.jpg)

GCN模型本质上是从每一层每个节点的邻域聚集特性。对于一个目标节点，对抗样本试图连接不同特征和标签的节点，以最大限度地改变聚合的邻居特征。相应地，当移除连边时，攻击倾向于删除连接与目标节点最相似的节点的边。正如上面所说的，**边攻击更有效，因为添加或删除一条边缘会影响聚合期间的所有特征维度。相比之下，只修改一个特征只影响特征向量的一个维度，其扰动很容易被其他高阶节点的邻居所掩盖。**

基于上述观察，论文做出了另一个假设，**在防御的过程中，模型给连接特征差异较大的节点之间的边分配了更低的权重。**为了验证上述假设，绘制了图2，显示了一个目标节点的所有边的jaccard相似性分数和学习到的权重。对于选择的目标节点，干净图中目标节点和它邻居节点之间的边的相似性分数都是大于0的。相似性分数为0的边都是攻击加上去的。如预期所料，模型为大多数相似性分数较低的边学习了较低的权重，因此能够减小扰动边对预测带来的影响。

![8f8b95ca07cac45f9a333d56ab8c3bb.png](http://ww1.sinaimg.cn/mw690/005NduT8ly1ga6pszoberj30gb0bl406.jpg)

为了使防御更有效，作者表示甚至不需要使用可训练的边权值作为防御。学习边权值必然会给模型引入额外的参数，从而影响GCN模型的可扩展性和准确性。一种潜在有效的简单方法是注意到在大多数数据集中节点通常不连接到没有特征相似的节点。此外，基于学习的防御技术本质上为连接两个不同节点的边缘分配了较低的权重。

那么论文提出了一种图的预处理方法作为防御手段。删除图中相似性分数较低的边。论文中表示，干净图中这些相似度较低的边数量较少，删掉这些边对GCN模型的预测性能影响不大。在对抗样本中，很多相似度较低的边是在攻击过程中加入的扰动，删掉这些边可以过滤掉一些噪声，提高模型的预测性能。这种防御策略简单且便于计算，只需要对图进行一次遍历。





## Evaluation

论文使用了三个数据集进行评估：

| Dataset  | Nodes | Features | Edges |
| -------- | ----- | -------- | ----- |
| CORA-ML  | 2708  | 1433     | 5429  |
| Citeseer | 3327  | 3703     | 4732  |
| Polblogs | 1490  | -        | 19025 |

数据集划分：20%标注（10%训练，10%测试），80%未标注。对于Polblogs数据集，因为没有特征属性，故用单位矩阵代替特征矩阵。



### Transductive Attack

如前所述，由于半监督或直推设置，模型攻击时被认为是不固定的。在对特征或边进行扰动后，对模型进行再训练，以评估攻击的有效性。

为了验证攻击的有效性，实验选择了不同置信度分数的节点。具体来说，总共选择了40个节点，其中包含10个得分最高的节点、10个得分最低的节点和20个随机选择的节点。将提出的IG-JSMA与几种baseline进行比较，包括随机攻击、FGSM和nettack。

为了评估攻击的有效性，引入了分类边界的概念。对于一个目标节点$v$，$v$的分类边界被定义为


$$
Z_{v, c}-\max _{c' \neq c} Z_{v, c'}
$$


$c$为真实类标，$Z_{v,c}$为节点$v$被预测为$c$的概率，分类边界也就是节点$v$被预测为真的概率与预测为错的最大概率之差。越低的分类边界表明攻击效果越好。图3表示在修改后的图上对模型重新训练后的节点分类边界。IG-JSMA性能优于baseline，同时分类边界的方差较小表明性能稳定。

![fd1926a05f4baf275b203adde0995d6.png](http://ww1.sinaimg.cn/large/005NduT8ly1ga6r5ysk64j30x20dnq4f.jpg)

表2显示了，两者攻击方法对比之下的正确分类的节点比例。

![dd6f77ee52eb83b286be907cc159892.png](http://ww1.sinaimg.cn/mw690/005NduT8ly1ga6rob35o4j30g704g0sy.jpg)

对于图中的一个目标节点，给定一个两层的GCN模型，目标节点的预测只依赖于它的2-hop ego-net。我们定义特征/边的重要性如下:对目标节点$v$，暴力方法测量节点和边的重要性，一次删除一个节点或一条边，并检查目标节点的预测评分的变化。

假设期望类$c$的预测分数为$p_c$，当移除边$(i,j)$时，$p_c$变为$p'_c$，定义边的重要性为$\Delta_{p_{c}}=p_{c}^{\prime}-p_{c}$。衡量一个节点的重要性，移除和该节点连接的所有边，观察预测分数的变化。节点重要性可以近似用预测分数关于所有节点特征的梯度之和，或者关于所有邻接矩阵元素的梯度之和表示。



![1cb72019b7e0da5bff0157de6e480a8.png](http://ww1.sinaimg.cn/large/005NduT8ly1ga6s7xu957j30vi0d3acl.jpg)

图中节点颜色表示不同的类，圆形节点表示有正的重要性分数，菱形节点表示有负的重要性分数，节点大小表示重要性数值的大小。红色边有正的重要性分数，蓝色边有负的重要性分数，边的粗细表示重要性数值的大小。五角星表示了攻击的目标节点。

- 图4b是通过普通梯度攻击生成的，可以看出几乎所有边都有非0的重要性分数，所有很难看出节点和边的相对重要性。
- 图4a是通过暴力求解的，相比之下可以看出对于，目标节点而言，大量的边都是不重要的。
- 图4c是通过积分梯度攻击生成的，与暴力求解很近似。



### Defense

研究在不同情况下所提出的防御技术的有效性。使用具有节点特征的CORA-ML和Citeseer数据集。首先评估所提出的防御方法是否会影响模型的性能。表3显示了有/没有防御的GCN模型的准确性。我们发现，提出的防御方法作为防御方法的预处理，成本低廉，几乎不会对GCN模型的性能产生任何负面影响。

![61dd42c199177d11773084b9f3b8500.png](http://ww1.sinaimg.cn/large/005NduT8ly1ga6sp7jkjgj30gi05jt98.jpg)

对于不同的攻击，评估被攻击节点的分类边界和准确率在防御前后的变化。与直推攻击实验一样，我们选择了40个预测分数不同的节点。结果见表4。首先，在没有防御的情况下，大多数被选择的节点被错误地分类，因为对任何攻击accuracy都在0.05以下。通过启用防御方法，无论采用何种攻击方法，都可以显著提高准确性。这在一定程度上说明了所有的攻击方法都是寻找相似的边进行攻击，所提出的防御方法是攻击无关的。虽然仍有少数节点被防御方误分类，但由于分类边界的增大，其期望类的预测置信度较低。因此，欺骗用户变得更加困难，因为手动检查通常涉及预测的低可信度。总的来说，尽管只删除连接节点的Jaccard相似度评分为0的边缘，但是所提出的防御是有效的。

![fedd19ca8ce3172c68ee76db24c49f0.png](http://ww1.sinaimg.cn/large/005NduT8ly1ga6t8ylhrzj30gr0943zs.jpg)