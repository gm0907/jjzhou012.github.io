---
layout: post
title: node2vec的实现与思考
date: 2019-04-26 00:08:00 +0800
tag: graph embedding
categories: blog
pageview: true
---



# Node2vec的实现与应用



## 写在前面

我们生活的世界中存在着各种各样的网络，从宏观角度的全球物流、通信、交通网络，到微观世界的蛋白质结构、化合物作用网络，从交织于现实与虚拟世界的社交网络，到学科领域的科学家合作网络。网络系统的存在是世界复杂性的体现，延伸出了一门新的交叉学科——网络科学。

<!--more-->

![](http://ww1.sinaimg.cn/large/005NduT8ly1g2imjgfj1fj30hu0amdqc.jpg)

网络科学是从交叉学科研究成长起来的一个新兴的学术领域。致力于研究复杂网络的性质，并且应用这些性质去研究一些具有网络特点的领域，比如信息技术网络，计算机网络，生物圈网络，学习和认知网络，社会关系网络以及经济和金融网络。这个领域以数学中的图论为理论基础，从物理中的统计力学，计算机科学中的数据挖掘和信息可视化，统计学中的推断建模，以及社会学和经济学中的社会结构理论等学科和分支中汲取方法论营养。

## 一、研究背景

### 网络的发展

复杂网络本质上是基于数据的复杂系统，探索网络研究的先进方法论对于学科的进步尤为重要。目前计算机领域中最火热的当属人工智能，其包含的机器学习、深度学习模型与方法在大数据的支持下，摧枯拉朽般的实现了困扰人类多年的许多任务，使得许多机器辅助功能都变得可实现，这是传统统计学方法无法实现和超越的。

复杂网络早期的方法论主要是基于统计学和图论，同时结合交叉领域的专业信息，像社团检测任务中的`infomap`、`betweenness`、`label propagation algorithm`，链路预测中的节点相似性指标`CN`、`RA`、`LP`等，这些方法在早期的时候发光发热，直到现在，其中一些方法依然不乏较好的效果。但随着大数据时代的到来，网络的形式、规模也有了巨大的发展，从原本的稀疏网络到现在上亿节点和关系的巨型网络，从原本的静态时不变网络到如今的动态分层网络，从原本同构的节点属性单一的网络到如今知识图谱等的节点属性丰富关系复杂的异构网络。原本的统计学方法在面对复杂的、特征多样的网络系统，似乎力不从心，如何探索更适合的方法论已成为网络科学的研究热点。

### 与AI结合前的思考

既然人工智能的热潮持续不衰，网络科学能否“蹭一波热度”，将形式多样、性能优良的机器学习、深度学习方法迁移到网络科学中呢，研究者就此开始了探索。

占据人工智能领域一大块的当属计算机视觉，也就是我们常说的图像识别。众所周知，图像由像素点构成，以2维的形式规则的排列，卷积核通过固定尺寸的窗口在图像上移动并提取特征。

![](http://ww1.sinaimg.cn/mw690/005NduT8ly1g2dirwh8usj30k4066429.jpg)

自然语言处理针对文本数据，流行的 word2vec 模型利用 skip-gram 在文本上移动提取词组特征。

![](http://ww1.sinaimg.cn/mw690/005NduT8ly1g2divubwtdj30fq08lq4g.jpg)

图像、文本数据中，我们可以很容易的定义数据元之间的距离，如像素之间的欧式距离，文本词汇之间的上下文关系等，通过距离的定义，我们可以进一步探索数据之间的相似性。以上我们可以称之为欧式数据，针对这些数据可以通过固定尺寸的核来提取特征用于训练。

在网络中呢？一般网络（图）在计算机中通常用一个邻接矩阵表示其结构信息（节点、连边），矩阵的行与列由网络节点构成，如果两个节点之间存在连边，那么矩阵对应坐标的元素设置为边权值（无权图设为1）。如果我们打乱节点顺序，重新构造邻接矩阵，新的邻接矩阵与原邻接矩阵等价，依然能反映网络的结构信息。

![](http://ww1.sinaimg.cn/mw690/005NduT8ly1g2dkjzap4aj30if0e6gmc.jpg)

但是问题由此产生，原邻接矩阵若我们定义节点1，2之间的欧式距离 $d_{1,2}=1$，在新的邻接矩阵中，$d_{1,2}=4$，所以在网络中我们无法用欧式距离来定义节点之间的距离；此外若我们想用类似卷积核的核窗口来提取网络特征，可以发现：

- 若中心节点为1，则`node1`的邻居节点有两个`node2`与`node4`;
- 若中心节点为3，则`node3`的邻居节点有三个`node2`、`node4`与`node5`;

因为核窗口应该包含中心点和其邻域点，但因网络中不同节点的邻居节点数存在差异，所以无法确定核窗口的尺寸，来完美囊括每一个节点的邻域节点。

由此我们可以看出，网络与图像、文本等最大的差异在于顺序性，图像、语料等数据规则且具有一定顺序性，而网络没有。网络存在于非欧式空间，网络数据属于非欧式数据，无法直接应用机器学习、深度学习里的一些特征提取方法。

### 突破

研究者继续探索，既然网络没有顺序性，能否通过构造一定的顺序性，来反映网络的拓扑结构，继而衔接机器学习算法呢？由此，网络中的随机游走（random walk）概念被提出，继而引发了新的热潮。

什么是网络中的random walk？区别于全局优化中的random walk算法，网络中的random walk是一种采样策略，在RW的每一步采样中，从当前节点$v$的邻居节点中随机地选取一个节点$w$作为下一个待采样节点。从节点$v$随机游走到其邻居节点$w$的概率与当前节点$v$的度相关：


$$
P_{v, w}^{R W}=\left\{\begin{array}{c}{\frac{1}{d(v)}, \text { if } w \text { is a neighbor of } v} \\ {0, \text { otherwise }}\end{array}\right.
$$


通过随机游走采样，我们可以获取到一系列的节点序列，这些节点序列是有顺序性的，我们可以把游走生成的序列类比成文本中的sentence，序列中的每一个节点看作句子中的word：

- 节点序列 $\Longrightarrow$ sentence
- node $\Longrightarrow$ word

![](http://ww1.sinaimg.cn/mw690/005NduT8ly1g2dogx8tv9j30gb081dh2.jpg)



### 方法

既然可以通过random walk采样得到具有类文本性质的节点序列，那么我们是否可以利用nlp中的模型和算法来对网络进行研究呢？

研究者提出了Deepwalk模型，Deepwalk的思想非常简单，首先将网络结构通过随机游走采样的方式，转换为类似“sentence”的节点序列的形式，再通过word2vec中的Skip-gram模型或者CBOW模型，训练得到每个节点的向量表示形式，进而可以用余弦距离或者欧式距离来求得两个节点之间的相似度。



## 二、Node2vec原理

整体上来看，Node2Vector在deepwalk的基础上改变了节点游走的方式，考虑了更多的信息。Deepwalk在下一个节点的选择上只是从当前节点的所有邻居节点中随机选择一个，而node2vec结合传统的BFS和DFS的两种搜索方法，提出了新的搜索策略。

### 前提假设

Node2vec将网络特征学习视为一个极大似然问题。对于一个给定的网络 $G=(V, E)$ ，设计一个函数映射 $f : V->R^{d}$ 将特征映射为 $d$ 维向量。对于网络中的任一节点 $v \in V$ ， 利用 采样策略 $s$ 得到的节点 $v$ 的邻域信息集合为 $N_{S}(u) \subset V$ 。整个特征学习过程为一个极大似然优化问题：

$$
\max _{f} \sum_{u \in V} \log \operatorname{Pr}\left(N_{S}(u) | f(u)\right)
$$


为了简化该优化过程，作者提出了两个前提假设：

- 条件独立性假设：给定源节点的特征表示，观察到一个邻节点的似然，与观察到其它邻节点的似然是相互独立的；
  
  
  $$
\operatorname{Pr}\left(N_{s}(u) | f(u)\right)=\prod_{n_{i} \in N_{S}(u)} \operatorname{Pr}\left(n_{i} | f(u)\right)
  $$
  
- 特征空间的对称性：源节点与邻域节点在特征空间上具有对称性；
  
  
  $$
  \operatorname{Pr}\left(n_{i} | f(u)\right)=\frac{\exp \left(f\left(n_{i}\right) \cdot f(u)\right)}{\sum_{v \in V} \exp (f(v) \cdot f(u))}
  $$

基于以上两个假设，问题转化为


$$
\begin{array}{c}{\max _{f} \sum_{u \in V}\left[-\log Z_{u}+\sum_{n_{i} \in N_{S}(u)} f\left(n_{i}\right) \cdot f(u)\right]} \\ {Z_{u}=\sum_{v \in V} \exp (f(u) \cdot f(v))}\end{array}
$$



### 网络中的BFS与DFS

![](http://ww1.sinaimg.cn/mw690/005NduT8ly1g2dribz0kgj30kg07zjte.jpg)

在网络中，图嵌入学习得到的向量，应该需要表示两种信息：

- 结构相似性（结构等价）：若网络中两个节点（直接相连）共享许多邻居节点，那么这两个节点具有结构相似性；如上图中的$u$和$s_1$ ，它们共享相同的邻居节点$s_2$和$s_3$，它们具有结构相似性；
- 同质性（规则等价）：若网络中两个节点（不直接相连）在网络中扮演相似的角色，即它们所处的网络结构相似，那么这两个节点为规则等价；如上图中的$u$和$s_6$ ，它们所处的网络结构相似，都有四个邻居节点，且邻居节点之间也存在一定连接，它们具有同质性；

传统的BFS和DFS在网络中的表现：

- 广度优先搜索(BFS)：在节点周围搜索，探索局部的结构形态，邻域$N_s$仅限于源节点的近邻节点；善于发现节点之间的结构相似性；
- 深度优先搜索(DFS)：在更大的范围去搜索高阶的结构形态，邻域由距离源节点越来越远的顺序采样的节点组成；善于发现节点之间的规则相似性；



### Random Walks

给定一个源节点$u$，我们模拟一个固定长度$l$的随机游走。让$c_i$表示游走中的第$i$个节点，从$c_0=u$ 开始 ，节点 $c_i$以如下概率分布生成：

$$
P\left(c_{i}=x | c_{i-1}=v\right)=\left\{\begin{array}{ll}{\frac{\pi_{v x}}{Z}} & {\text { if }(v, x) \in E} \\ {0} & {\text { otherwise }}\end{array}\right.
$$


其中 $\pi_{vx}$ 为节点$v$和$x$之间的非标准化转移概率，$Z$是标准化常数。

随机游走相比于BFD/DFS，有以下优势：

- 时间复杂度：通过在样本生成过程中引入图的连通性，随机游走提供了一种机制，通过跨不同源节点重用样本来提高有效采样效率。

最简单的随机游走方法就是让$ \pi_{vx} = w_{vx}$，就是按边的权重分配等价的概率来选择下一个节点。但是这样的设置是无法较好的搜索节点的邻域信息的。



### Node2vec的搜索策略

Node2vec结合BFS和DFS的优势，改变了deepwalk中的random walk搜索策略，提出了二阶的随机游走策略，通过两个参数$p,q$来控制。

![](http://ww1.sinaimg.cn/mw690/005NduT8ly1g2drx9rii3j30br08d75f.jpg)

考虑一个随机游走过程，它刚刚遍历了边 $(t, v)$，现在驻留在节点$v$(图)，游走策略需要确定下一个节点走哪里，这里设置未归一化转移概率为 $
\pi_{v x}=\alpha_{p q}(t, x) \cdot w_{v x}
$ ，其中收缩偏置$\alpha$定义如下：

$$
\alpha_{p q}(t, x)=\left\{\begin{array}{ll}{\frac{1}{p}} & {\text { if } d_{t x}=0} \\ {1} & {\text { if } d_{t x}=1} \\ {\frac{1}{q}} & {\text { if } d_{t x}=2}\end{array}\right.
$$

其中 $d_{tx}$ 代表了节点 $t$ 与 $x$ 之间的最短路径，$d_{tx} \in \{0,1,2\}$ 

参数：

- Return parameter：参数$p$控制在游走过程中访问上一个节点的概率, $\frac{1}{p}$, 提高$p$值可以降低在两个采样过程中重新访问已经访问过的节点的概率, 避免2-hop冗余；
- In-out parameter：参数$q$允许搜索区分"向内"和"向外"节点, $q>1$, 搜索为BFS; $q<1$, 搜索为DFS；



### 算法流程

![](http://ww1.sinaimg.cn/mw690/005NduT8ly1g2dsml9u6mj30c00ba3zr.jpg)

Node2vec的三个主要阶段：

- 转移概率初始化
- 模拟随机游走采样
- 基于word2vec的SGD优化

过程具体说明如下：

- **转移概率初始化**：采样是基于$\pi_{vx}$的转移概率，对于二阶马尔可夫链的转移概率需要预先计算。这里需要注意，转移概率有两部分：

  - **节点的转移概率**：节点的转移概率用于第一次游走，因为第一次游走采样之前只确定了序列初始节点，不存在边的信息，故无法利用前一次采样的边来执行二阶的搜索采样，所以第一次游走采样的节点需要通过节点之间的转移概率来确定。节点之间的转移概率基于边的权值，即令$ \pi_{vx} = w_{vx}$。

    ![](http://ww1.sinaimg.cn/large/005NduT8ly1g2dtazaru9j30s5083aar.jpg)

  - **边的转移概率**：后续的游走属于$p,q$控制的二阶搜索采样过程，使用边级别的转移概率。

    ![](http://ww1.sinaimg.cn/large/005NduT8ly1g2dtbfc8e2j30ms0f93zu.jpg)

  > 注意：初始化转移概率的过程其实是[alias sample](<http://www.keithschwarz.com/darts-dice-coins/>)的第一阶段，初始化alias table的过程。该过程时间复杂度为$O(n)$。
  >
  > 具体算法实现：
  >
  > ![](http://ww1.sinaimg.cn/large/005NduT8ly1g2dtjxpy5qj30rh0nhmzg.jpg)

  

- **模拟随机游走采样**：该过程属于alias sample(别名采样)的第二阶段，采样过程。该过程基于alias sample的第一阶段生成的alias table。

  在随机游走过程中，由于起始节点的选择不同，会引入潜在偏差。由于我们需要学习所有节点的表示，我们通过模拟从每个节点开始的$r$个长度为$l$的随机游走来抵消这个偏差。

  ![](http://ww1.sinaimg.cn/large/005NduT8ly1g2dtn2ieg2j30sa0armxx.jpg)



- **基于word2vec的SGD优化**：该过程将采样得到的节点序列输入word2vec模型，使用随机梯度下降优化损失函数，得到节点的向量表示。

  ![](http://ww1.sinaimg.cn/large/005NduT8ly1g2dtsz9tjqj30x207caah.jpg)



最终，我们得到节点的向量表示，可以用于下游任务。

![](http://ww1.sinaimg.cn/large/005NduT8ly1g2du8jinlcj30r603yjro.jpg)





## 三、实验

我们通过node2vec表征了网络节点，得到了每个结点的向量表示。至此，我们可以衔接机器学习算法，实现下游的任务。

我们设计了两个实验，分别为了说明不同的目的：

- 社团检测
- 节点分类

### 社团检测

#### **社团检测介绍**

社团结构是除小世界、无标度特性外复杂网络研究领域的又一重大发现。所谓的社团结构，就是网络中紧密连接的子图，子图内部的连边密度高于子图之间的连边密度。网络中存在的社团结构可以揭示出一定的行为特点和功能，展示系统拓扑结构中蕴含的信息。社团检测算法的研究就是为了挖掘出网络中存在的社团结构。

而Node2vec等图嵌入算法的出现为解决社团检测问题提供了一个新思路。

#### 评价指标

衡量社团检测算法检测到的社团结果质量也是该领域研究中的一个重点。该领域的研究者们先后提出了一些评价指标，主要分为真实社团划分结果未知和真实社团划分结果已知两种情况，前者包括模块度及其相关扩展，后者包括标准化互信息（Normalized Mutual Information, NMI），调整兰德指数（Adjusted Rand Index, ARI）等。

本文选择了标准化互信息NMI作为评价指标，它是一个基于信息论的评价指标，衡量了检测到的社团划分结果和真实划分结果之间的相似性。将检测到的社团划分结果和真实的划分结果分布用随机变量$X$和$Y$表示，$x_i$表示检测到的划分结果中节点$i$所属的社团标号，$y_i$表示真实划分结果中节点$i$所属的社团标号。随机变量$X$和$Y$的互信息定义为：

$$
I(X, Y)=\sum_{x y} P(x, y) \log \frac{P(x, y)}{P(x) P(y)}
$$

其中，$P(x, y)=P(X=x, Y=y)=n_{x y} / n$为随机变量$X$和$Y$的联合分布，同样有$P(x)=P(X=x)=n_{x} / n$和$P(x)=P(X=x)=n_{y} / n$。

进一步将其归一化得到：

$$
I_{\text {norm}}(X, Y)=\frac{2 I(X, Y)}{H(X)+H(Y)}
$$

其中，$H(x)=-\sum_{x} P(x) \log P(x)$为划分结果的信息熵，$H(Y)$同样。

#### 数据集说明

本章实验设计的过程中选取了四个常用于社团检测的数据集，包括空手道俱乐部网络、海豚社交网络、美国大学橄榄球联赛网络以及美国政治书网络，数据集基本信息如下表所示。

- 空手道俱乐部网络：该网络是由Zachary历时两年观察一个空手道俱乐部中的日常所构建的社交网络，网络中的节点表示俱乐部中的成员，连边表示成员之间存在的朋友关系。后因为俱乐部的教练和管理者之间发生分歧，最终一分为二。
- 海豚社交网络：该网络是由Lusseau 通过对新西兰神奇湾的海豚长达7 年的观察构建而成。网络中的节点代表了海豚，连边代表两只海豚间具有频繁联系。该网络后因一只海豚的离去划分成了两个社团。
- 美国大学橄榄球联赛网络：该网络表示的是是 2000 赛季组比赛的赛事安排，网络中的节点表示各个大学代表队，连边表示两个队伍之间开展过常规赛。所有参赛队伍被划分到了12 个联盟。相较于隶属不同联盟的队伍，同属于一个联盟的队伍之间会有更频繁的赛事，使得该网络表现出一定的社团结构。
- 美国政治书网络：V.Krebs根据亚马逊网站上在 2004 年美国总统大选期间销售的与政治相关的书籍构建了美国政治书网络。其中节点表示亚马逊上销售的政治书，连边表示有读者同时购买了两本书籍。

| 数据集                 | 节点数 | 连边数 | 真实社团个数 |
| ---------------------- | ------ | ------ | ------------ |
| 空手道俱乐部网络       | 34     | 78     | 2            |
| 海豚社交网络           | 62     | 159    | 2            |
| 美国大学橄榄球联赛网络 | 115    | 613    | 12           |
| 美国政治书网络         | 105    | 441    | 3            |

#### 对比算法

- 标签传播算法 (LPA)：Raghavan等人提出的LPA算法的基本思想为，算法初始为每个节点各自分配一个标签，然后对节点的标签进行迭代更新，更新规则为将节点新的标签设定为其邻接节点标签中出现频率最高的一个，出现相同最大值是则进行随机选择，当没有节点的标签再发生变化时，算法终止。
- Infomap算法(INF)：Rosvall等人提出的Infomap算法主要基于网络图和编码理论。它通过压缩网络中的信息流描述来检测社团。因为信息流在经过不同社团时会产生更大的编码长度，其关键就在于最小化平均描述长度L（M）。
- Louvain算法 (LOU)：Blondel等人提出的Louvain算法将社团检测视为一个多级模块度优化问题，主要包括两个步骤。第一阶段它先从孤立的节点开始，重复将节点划分到一个社区，实现最大化模块度增益，直到无法实现进一步的改进。第二阶段它再将此前检测到的社团合并为超级节点构建新网络。重复执行两个阶段直到算法稳定。
- Node2vec+K-Means：Node2vec的原理以及算法流程已经在第二章中详细介绍，这里主要介绍K-Means聚类算法。该算法的实现流程为先随机选取K个对象作为初始的聚类中心，然后计算每个对象和各聚类中心之间的距离，将其分配给与它最接近的聚类中心。完成一轮分配后重新计算聚类中心，重复上述流程直至满足某个终止条件，得到最后的聚类结果。



#### 实验结果及分析

我们在4个数据集上均做了实验，对比了Node2vec+K-Means和其余3种传统的社团检测算法的效果，实验结果如下表所示，表中我们标出了每个数据集上性能最优的两个结果。可以看到，Node2vec+K-Means在这4个数据集上均表现出了较好的检测准确性。

|                  | Karate    | Dolphins  | Football  | Polbooks  |
| ---------------- | --------- | --------- | --------- | --------- |
| LPA              | **0.806** | **0.607** | 0.888     | **0.586** |
| Infomap          | 0.699     | 0.553     | **0.924** | 0.493     |
| Louvain          | 0.587     | 0.511     | 0.890     | 0.512     |
| Node2vec+K-Means | **0.951** | **0.889** | **0.922** | **0.564** |

Node2vec等图嵌入算法的提出，提供了将网络从非欧空间转移到向量空间的新思路，由此，针对网络的算法研究不再仅仅依赖于网络拓扑结构，表征后得到的节点向量可以反映网络中节点间的相关性，用于后续的机器学习算法，可以在下游任务中得到优于传统方法的效果。



### 节点分类

#### 评价指标

分类是监督学习中的一个核心问题。为了评价一个分类器的分类性能优劣，需要引入一些评估指标，常用的一些指标有准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F值等。

在二分类问题中，通常以关注的类为正类，其他类为负类，依据分类器在测试数据集上的预测正确与否，四种情况出现的总数分别记作：

- TP：将正类预测为正类的样本数；
- FN：将正类预测为负类的样本数；
- FP：将负类预测为正类的样本数；
- TN：将负类预测为负类的样本数；

其中，True、False表示分类正确与错误，Positive、Negative表示正、负样本。

微平均（Micro-averaging）是对数据集中的每一个示例不分类别进行统计建立全局混淆矩阵，然后计算相应的指标。其计算公式如下：

micro精度(precision)：

$$
P_{\text {micto}}=\frac{T \overline{P}}{T P+F \overline{P}}=\frac{\sum_{i=1}^{n} T P_{i}}{\sum_{i=1}^{n} T P_{i}+\sum_{i=1}^{n} F P_{i}}
$$
micro召回(recall)：

$$
R_{\text {micro}}=\frac{T^{\overline{P}}}{T \overline{P}+F \overline{N}}=\frac{\sum_{i=1}^{n} T P_{i}}{\sum_{i=1}^{n} T P_{i}+\sum_{i=1}^{n} F N_{i}}
$$
micro-F1：

$$
F_{\text {micro}}=\frac{2 \times P_{\text {micro}} \times R_{\text {micro}}}{P_{\text {micro}}+R_{\text {micro}}}
$$
Micro-averaging赋予每个样本决策相同的权重，对于数据集中各个类的分布不平衡的问题，更建议使用mirco-F1，因为macro没有考虑到各个类别的样本大小。



#### 数据集说明

- BlogCatalog：BlogCatalog网站上列出的博主的社交关系网络。标签表示通过博主提供的元数据推断的博主兴趣。该网络有10,312个节点，333,983个边缘和39个不同的标签。
- Wikipedia:  这是一个由出现在Wikipedia转储文件的前一百万字节中的单词组成的并发网络。标签表示使用Stanford post - tagger推断的词性(POS)标签。该网络有4777个节点、184812条边和40个不同的标签。

#### 对比算法

- Deepwalk: DeepWalk通过**截断随机游走**(truncated random walk)学习出一个网络的**社会表示**，其中的采样策略可以看作是node2vec的一个特例，p = 1, q = 1。

- LINE：利用了网络的一阶和二阶相似性，能够保留局部和全局的网络结构。同时提出了边缘采样方法，解决了经典随机梯度下降的局限性，提高了算法的有效性和效率。能够适用于任意类型的网络。

#### 实验结果与分析

我们在两个数据集上做了实验，对比了三种图嵌入算法在同一节点分类任务中的表征效果。我们按数据集的划分比例，从10%-90%，将数据集划分为训练集和测试集，每种划分方式进行5次实验取平均。实验结果如下图所示，node2vec和deepwalk在这两种数据集上的表征效果相近，但都优于LINE方法。LINE方法适用于大规模网络，在实验用的两个数据集上效果没有其他两种算法好。

![](http://ww1.sinaimg.cn/large/005NduT8ly1g2ic19o6kej30y10eomzh.jpg)

不同的图嵌入方法，从不同角度对网络的拓扑结构进行表征，且适用于不同规模和类型的网络。



## 写在最后

图嵌入算法的提出，将网络科学与机器学习联系起来，带来了新的思路。

