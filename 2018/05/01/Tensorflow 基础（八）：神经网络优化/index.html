<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="神经网络优化 学习率设置   过拟合问题   滑动平均模型  学习率设置在梯度下降算法中，对于参数 $\theta$，其梯度为 $\frac{\partial}{\partial \theta}J(\theta)$ 。还需要定义一个学习率 $\eta$ (learning rate)来定义每次参数更新的幅度。通过参数的梯度和学习率，参数更新公式为：   $$\theta _{n+1} =\thet">
<meta name="keywords" content="Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow 基础（八）： 神经网络优化">
<meta property="og:url" content="http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/index.html">
<meta property="og:site_name" content="Andy_z &#39;s Blog">
<meta property="og:description" content="神经网络优化 学习率设置   过拟合问题   滑动平均模型  学习率设置在梯度下降算法中，对于参数 $\theta$，其梯度为 $\frac{\partial}{\partial \theta}J(\theta)$ 。还需要定义一个学习率 $\eta$ (learning rate)来定义每次参数更新的幅度。通过参数的梯度和学习率，参数更新公式为：   $$\theta _{n+1} =\thet">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://p5bxip6n0.bkt.clouddn.com/18-4-29/64880644.jpg">
<meta property="og:image" content="http://p5bxip6n0.bkt.clouddn.com/18-4-30/788202.jpg">
<meta property="og:image" content="http://p5bxip6n0.bkt.clouddn.com/18-5-1/6239431.jpg">
<meta property="og:image" content="http://p5bxip6n0.bkt.clouddn.com/18-5-1/96305551.jpg">
<meta property="og:updated_time" content="2018-05-01T08:07:26.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow 基础（八）： 神经网络优化">
<meta name="twitter:description" content="神经网络优化 学习率设置   过拟合问题   滑动平均模型  学习率设置在梯度下降算法中，对于参数 $\theta$，其梯度为 $\frac{\partial}{\partial \theta}J(\theta)$ 。还需要定义一个学习率 $\eta$ (learning rate)来定义每次参数更新的幅度。通过参数的梯度和学习率，参数更新公式为：   $$\theta _{n+1} =\thet">
<meta name="twitter:image" content="http://p5bxip6n0.bkt.clouddn.com/18-4-29/64880644.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Tensorflow 基础（八）： 神经网络优化</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/07/15/01-HTML/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2018/04/25/reveal.js/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Post Anterior</span>
      <span id="i-next" class="info" style="display:none;">Post Següent</span>
      <span id="i-top" class="info" style="display:none;">Adalt</span>
      <span id="i-share" class="info" style="display:none;">Compartir Post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&text=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&is_video=false&description=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Tensorflow 基础（八）： 神经网络优化&body=Check out this article: http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&name=Tensorflow 基础（八）： 神经网络优化&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#神经网络优化"><span class="toc-number">1.</span> <span class="toc-text">神经网络优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#学习率设置"><span class="toc-number">1.1.</span> <span class="toc-text">学习率设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#过拟合问题"><span class="toc-number">1.2.</span> <span class="toc-text">过拟合问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Early-stopping"><span class="toc-number">1.2.1.</span> <span class="toc-text">Early stopping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集扩增（Data-augmentation）"><span class="toc-number">1.2.2.</span> <span class="toc-text">数据集扩增（Data augmentation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正则化-（regularization）"><span class="toc-number">1.2.3.</span> <span class="toc-text">正则化 （regularization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-number">1.2.4.</span> <span class="toc-text">Dropout</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#滑动平均模型"><span class="toc-number">1.3.</span> <span class="toc-text">滑动平均模型</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Tensorflow 基础（八）： 神经网络优化
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Andy_z 's Blog</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2018-04-30T16:00:00.000Z" itemprop="datePublished">2018-05-01</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Tensorflow/">Tensorflow</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Tensorflow/">Tensorflow</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="神经网络优化"><a href="#神经网络优化" class="headerlink" title="神经网络优化"></a>神经网络优化</h1><ul>
<li>学习率设置  </li>
<li>过拟合问题  </li>
<li>滑动平均模型</li>
</ul>
<h2 id="学习率设置"><a href="#学习率设置" class="headerlink" title="学习率设置"></a>学习率设置</h2><p>在梯度下降算法中，对于参数 $\theta$，其梯度为 $\frac{\partial}{\partial \theta}J(\theta)$ 。还需要定义一个学习率 $\eta$ (learning rate)来定义每次参数更新的幅度。通过参数的梯度和学习率，参数更新公式为：  </p>
<p>$$<br>\theta _{n+1} =\theta _n - \eta \frac{\partial}{\partial \theta _n} J(\theta _n)<br>$$  </p>
<p>如果学习率过大，可能会导致参数在极优值两侧来回移动；如果学习率过小，虽然能保证收敛性，但会大大降低优化速度。<br>为解决学习率的问题，Tensorflow 提供了一种更加灵活的学习率设置方法—指数衰减法。通过这个函数可以使用较大的学习率快速得到一个较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.train.exponential_decay(</span><br><span class="line">    learning_rate,          <span class="comment"># 初始学习率</span></span><br><span class="line">    global_step,            <span class="comment"># </span></span><br><span class="line">    decay_steps,            <span class="comment"># 衰减速度， 完整使用一边训练数据所需要的迭代轮数，  decay_steps =  dataset_size / batch_size</span></span><br><span class="line">    decay_rate,             <span class="comment"># 衰减系数</span></span><br><span class="line">    staircase=<span class="keyword">False</span>,        <span class="comment"># True：阶梯衰减（global_step / decay_steps 为整数）； False：连续衰减</span></span><br><span class="line">    name=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该函数会指数级的减小学习率，实现了以下代码功能</span></span><br><span class="line">decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</span><br></pre></td></tr></table></figure>
<p><img src="http://p5bxip6n0.bkt.clouddn.com/18-4-29/64880644.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 生成学习率</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(learning_rate=<span class="number">0.1</span>, global_step=global_step, decay_steps=<span class="number">100</span>, decay_rate=<span class="number">0.96</span>, staircase=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 使用指数衰减的学习率，在minimize函数中传入global_step将自动更新该参数，从而使得学习率得到相应更新</span></span><br><span class="line">learning_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(...my loss..., global_step=global_step)</span><br></pre></td></tr></table></figure>
<h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>过拟合指的是模型对于训练数据拟合程度过度的情况。</p>
<p>当某个模型过度的学习训练数据中的细节和噪音，以至于模型在新的数据上表现很差，我们称发生了<strong>过拟合</strong>。这意味着训练数据中的噪音或者随机波动也被当做概念被模型学习了,这会<strong>导致模型比较复杂</strong>。而问题就在于这些概念不适用于新的数据，从而导致模型<strong>泛化性能变差</strong>。  </p>
<p><img src="http://p5bxip6n0.bkt.clouddn.com/18-4-30/788202.jpg" alt="">  </p>
<p>为了防止过拟合，我们需要用到一些方法，如：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）、Dropout等。</p>
<h3 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h3><p>对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。Early stopping便是一种<strong>迭代次数截断</strong>的方法来防止过拟合，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。  </p>
<p>Early stopping方法的具体做法是：  </p>
<ul>
<li>在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算验证集的 accuracy，    </li>
<li>在训练的过程中，记录到目前为止最好的 validation accuracy，当连续10次 Epoch（或者更多次）没达到最佳 accuracy 时，则可以认为 accuracy 不再提高了。此时便可以停止迭代了（Early Stopping）。<br>这种策略也称为“No-improvement-in-n”， n 即 Epoch 的次数，可以根据实际情况取，如10、20、30……  </li>
</ul>
<h3 id="数据集扩增（Data-augmentation）"><a href="#数据集扩增（Data-augmentation）" class="headerlink" title="数据集扩增（Data augmentation）"></a>数据集扩增（Data augmentation）</h3><p>在数据挖掘领域流行着这样的一句话，“有时候往往拥有更多的数据胜过一个好的模型”。因为我们在使用训练数据训练模型，通过这个模型对将来的数据进行拟合，前提是，<strong>训练数据与测试数据是独立同分布的</strong>。更多的数据往往使估计与模拟更准确。<br>通俗得讲，数据集扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。一般有以下方法：  </p>
<ul>
<li>从数据源采集更多数据  </li>
<li>复制原有数据加噪声  </li>
<li>重采样  </li>
<li>根据当前数据集估计数据分布参数，使用该分布产生更多数据等</li>
</ul>
<h3 id="正则化-（regularization）"><a href="#正则化-（regularization）" class="headerlink" title="正则化 （regularization）"></a>正则化 （regularization）</h3><p>正则化的本质：<strong>约束（限制）要优化的参数</strong>。<br>正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小 $\theta (j)$ ）。</p>
<p>这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。</p>
<p><a href="https://jjzhou012.github.io/2017/08/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E6%AD%A3%E5%88%99%E5%8C%96/" target="_blank" rel="noopener">正则化</a>  </p>
<ul>
<li>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。  </li>
<li>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。<br>实践中，可以将L1，L2正则化同时使用：  </li>
</ul>
<p>$$<br>R(w)=\sum_i \alpha \left|w_i \right|+(1-\alpha)w_i^2<br>$$  </p>
<p>Tensorflow 可以优化带正则化的损失函数。以下给出一个简单的带L2正则化的损失函数定义：    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line"><span class="comment"># loss 由两部分组成，第一部分为均方误差损失函数，刻画了模型在训练数据上的表现；  </span></span><br><span class="line"><span class="comment"># 第二部分为正则化，防止模型过度模拟训练数据中的随机噪声</span></span><br><span class="line"><span class="comment"># lambda 为正则化项的权重，w为需要计算正则化损失的参数</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(<span class="keyword">lambda</span>)(w)</span><br></pre></td></tr></table></figure>
<p>Tensorflow 提供了一些正则化函数：<br><img src="http://p5bxip6n0.bkt.clouddn.com/18-5-1/6239431.jpg" alt=""><br>以下是一些使用样例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">w = tf.constant([[<span class="number">1.0</span>, <span class="number">-2.0</span>], [<span class="number">-3.0</span>, <span class="number">4.0</span>]])  </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># L1正则化，输出为（|1|+|-2|+|-3|+|4|）* 0.5=5</span></span><br><span class="line">    print(sess.run(tf.contrib.layers.l1_regularizer(<span class="number">0.5</span>)(w)))</span><br><span class="line">    <span class="comment"># L2正则化， 输出为（1^2 + (-2)^2 + (-3)^2 + 4^2）/ 2 * 0.5 = 7.5</span></span><br><span class="line">    <span class="comment"># Tensorflow 会将L2正则化损失值除以2使得求导得到的结果更加简洁</span></span><br><span class="line">    print(sess.run(tf.contrib.layers.l2_regularizer(<span class="number">0.5</span>)(w)))</span><br><span class="line">    <span class="comment"># L1,L2同时使用</span></span><br><span class="line">    print(sess.run(tf.contrib.layers.l1_l2_regularizer(<span class="number">0.5</span>, <span class="number">0.5</span>)(w)))</span><br></pre></td></tr></table></figure>
<pre><code>5.0
7.5
12.5
</code></pre><p>在简单的神经网络中，这样的方式可以很好的计算带正则化的损失函数了。但当神经网络的参数增多之后，这样的方式可能导致损失函数loss的定义很长，可读性差且容易出错。更主要的是，当网络结构复杂之后定义网络结构的部分和计算损失函数的部分可能不在同一函数中，这样通过变量这样的方式计算损失函数就不方便了，为解决这个问题，可以使用 Tensorflow 中提供的集合（collection），它可以在一个计算图中保留一组实体。<br>下面代码给出了通过集合计算一个5层神经网络带L2正则化的损失函数的计算方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取一层神经网络边上的权重，并将这个权重的L2正则化损失加入名为‘losses’的集合中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, lambda)</span>:</span></span><br><span class="line">    <span class="comment"># 生成一个变量</span></span><br><span class="line">    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    <span class="comment"># add_to_collection函数将这个新生成变量的L2正则化损失项加入集合。</span></span><br><span class="line">    <span class="comment"># 这个函数的第一参数‘los''ses’是集合的名字，第二个参数是要加入集合的内容</span></span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(<span class="keyword">lambda</span>)(var))</span><br><span class="line">    <span class="comment"># 返回生成的变量</span></span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入输出</span></span><br><span class="line">x = tf.placeholder(dtype=tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(dtype=tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"><span class="comment"># 定义每一层网络的节点个数</span></span><br><span class="line">lay_dim = [<span class="number">2</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 神经网络层数</span></span><br><span class="line">n_layers = len(lay_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个变量维护前向传播时最深层的节点，开始为输入层。</span></span><br><span class="line">cur_layer = x</span><br><span class="line"><span class="comment"># 当前层的节点个数, 开始为输入层</span></span><br><span class="line">in_dim = lay_dim[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过一个循环来生成5层全连接的神经网络结构</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_layers):</span><br><span class="line">    <span class="comment"># 下一层的节点个数</span></span><br><span class="line">    out_dim = lay_dim[i]</span><br><span class="line">    <span class="comment"># 生成当前层中的权重变量，并将这个变量的L2正则化损失加入计算图上的集合</span></span><br><span class="line">    w = get_weight(shape=[in_dim, out_dim], <span class="number">0.001</span>)</span><br><span class="line">    bias = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[out_dim]))</span><br><span class="line">    <span class="comment"># 使用ReLU激活函数</span></span><br><span class="line">    cur_layer = tf.nn.relu(tf.matmul(cur_layer, w) + bias) </span><br><span class="line">    <span class="comment"># 进入下一层之前更新节点个数</span></span><br><span class="line">    in_dim = lay_dim[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在定义神经网络前向传播的同时已经将所有L2正则化损失加入了图上的集合  </span></span><br><span class="line"><span class="comment"># 这里只需要计算刻画模型在训练数据上表现的损失函数</span></span><br><span class="line">mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将均方误差损失函数加入损失集合</span></span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>, mse_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get_collection 得到一个列表，这个列表是所有这个集合中的元素，在这个样例中，</span></span><br><span class="line"><span class="comment"># 这些元素就是损失函数的不同部分，将它们加起来就可以得到最终的损失函数</span></span><br><span class="line">loss = tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br></pre></td></tr></table></figure>
<p>通过以上代码可以看出，通过使用集合的方法在网络结构比较复杂的情况下可以使代码可读性更高。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout是指在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。<br>DNNs是以概率 $p$ 舍弃部分神经元，其它神经元以概率 $q=1-p$ 被保留，舍去的神经元的输出都被设置为零。<br><img src="http://p5bxip6n0.bkt.clouddn.com/18-5-1/96305551.jpg" alt=""><br>上图为Dropout的可视化表示，左边是应用Dropout之前的网络，右边是应用了Dropout的同一个网络。  </p>
<p>dropout函数实现：引用keras的dropout实现源码  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, level)</span>:</span></span><br><span class="line">    <span class="comment"># level是概率值，必须在0~1之间  </span></span><br><span class="line">    <span class="keyword">if</span> level &lt; <span class="number">0</span> <span class="keyword">or</span> level &gt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Dropout level must be in interval [0, 1].'</span>)</span><br><span class="line">    retain_pron = <span class="number">1.</span> - level</span><br><span class="line">    <span class="comment"># 通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样  </span></span><br><span class="line">    <span class="comment"># 硬币正面的概率为p，n表示每个神经元试验的次数  </span></span><br><span class="line">    <span class="comment"># 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是神经元数量。  </span></span><br><span class="line">    <span class="comment"># 生成一个0，1分布的向量，0表示神经元被屏蔽，也就是dropout</span></span><br><span class="line">    sample = np.random.binomial(n=<span class="number">1</span>, p=retain_pron, size=x.shape)</span><br><span class="line">    print(sample)</span><br><span class="line">    <span class="comment"># 输入屏蔽部分神经元</span></span><br><span class="line">    x *=sample</span><br><span class="line">    print(x)</span><br><span class="line">    <span class="comment"># dropout后进行scale</span></span><br><span class="line">    x /= retain_pron</span><br><span class="line">    print(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试dropout， 输入向量x，经过dropout的结果</span></span><br><span class="line">x = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>], dtype=np.float32)</span><br><span class="line">dropout(x, <span class="number">0.4</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[0 1 0 1 1 1 1 0 1 1 0]
[0. 1. 0. 3. 4. 5. 6. 0. 8. 9. 0.]
[ 0.         1.6666666  0.         5.         6.6666665  8.333333
 10.         0.        13.333333  14.999999   0.       ]





array([ 0.       ,  1.6666666,  0.       ,  5.       ,  6.6666665,
        8.333333 , 10.       ,  0.       , 13.333333 , 14.999999 ,
        0.       ], dtype=float32)
</code></pre><p>为什么要进行rescale？<br>这被称为inverted dropout。当模型使用了dropout layer，训练的时候只有占比为 $p$ 的隐藏层单元参与训练，那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大 $p$ ，为了避免这种情况，就需要测试的时候将输出结果乘以 $\frac{1}{p}$ 使下一层的输入规模保持不变。而利用inverted dropout，我们可以在训练的时候直接将dropout后留下的权重扩大 $\frac{1}{p}$ 倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。</p>
<p>Tensorflow 中的 dropout：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">x = tf.Variable(tf.ones(shape=[<span class="number">10</span>], dtype=tf.float32))</span><br><span class="line">y = tf.nn.dropout(x, keep_prob=keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    </span><br><span class="line">    print(sess.run(y, feed_dict=&#123;keep_prob : <span class="number">0.4</span>&#125;))</span><br></pre></td></tr></table></figure>
<pre><code>[0.  2.5 2.5 0.  0.  2.5 0.  0.  2.5 0. ]
</code></pre><h2 id="滑动平均模型"><a href="#滑动平均模型" class="headerlink" title="滑动平均模型"></a>滑动平均模型</h2><p>滑动平均模型，它可以使得模型在测试数据上更鲁棒，在使用随机梯度下降算法训练神经网络时，通过滑动平均模型可以在一定程度上提高最终模型在测试数据上的表现。其实滑动平均模型，主要是通过控制衰减率来控制参数更新前后之间的差距，从而达到减缓参数的变化值（如，参数更新前是5，更新后的值是4，通过滑动平均模型之后，参数的值会在4到5之间），如果参数更新前后的值保持不变，通过滑动平均模型之后，参数的值仍然保持不变。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.train.ExponentialMovingAverage(</span><br><span class="line">    decay,                              <span class="comment"># 衰减率</span></span><br><span class="line">    num_updates=<span class="keyword">None</span>,                   <span class="comment"># 动态设置decay参数</span></span><br><span class="line">    zero_debias=<span class="keyword">False</span>, </span><br><span class="line">    name=<span class="string">'ExponentialMovingAverage'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>为了使得模型在训练的初始阶段更新得更快，ExponentialMovingAverage 还提供了 num_updates 参数来动态设置 decay 的大小，设置 num_updates 参数后，每次使用的衰减率为：<br>$$<br>decay= \min \left{decay,\  \frac{1 + num_updates}{10 + num_updates} \right}<br>$$<br>在 num_step 还比较小的时候，min() 会取到右边比较小的部分，也就是有一个比较小的 decay，这个时候模型更新会很快，当 step 增大时，模型更新速度会逐渐降低。</li>
<li>该函数对每一个待更新的变量（variable）都会维护一个影子变量（shadow variable）。影子变量的初始值就是这个变量的初始值，每次运行变量更新时，影子变量的值会更新为：<br>$$<br>shadow_variable = decay <em> shadow_variable + (1 - decay) </em> variable<br>$$<br>从公式可以看出，decay 决定了模型更新速度，decay 越大，模型越趋于稳定。实际运用中，decay 一般会设置为十分接近 1 的常数（0.999或0.9999）。  </li>
</ul>
<p>通过一段代码解释 ExponentialMovingAverage 是如何被使用的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个变量用于计算滑动平均，变量初始值为0。因为所有需要计算滑动平均的变量必须是实数型，所以指定变量类型为tf.float32</span></span><br><span class="line">v1 = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line"><span class="comment"># step 变量模拟神经网络迭代轮数，用于动态控制衰减率  ， 即 num_updates 参数</span></span><br><span class="line">step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个滑动平均类。初始化时给定衰减率decay和控制衰减率的变量step。</span></span><br><span class="line">ema = tf.train.ExponentialMovingAverage(decay=<span class="number">0.99</span>, num_updates=step)</span><br><span class="line"><span class="comment"># 定义一个更新变量滑动平均的操作。这里定义一个列表，每次执行该操作，列表中的变量会被更新。  </span></span><br><span class="line">maintain_averages_op = ema.apply([v1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 通过ema.average(v1)获取滑动平均之后变量的取值。</span></span><br><span class="line">    <span class="comment"># 初始化之后变量v1的值和v1的滑动平均都为0</span></span><br><span class="line">    print(<span class="string">'初始化变量和滑动平均值'</span>)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新变量v1的值到5</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">5</span>))</span><br><span class="line">    <span class="comment"># 更新v1的滑动平均值。衰减率为 min&#123;0.99 + (1+step)/(10+step)=0.1&#125;=0.1</span></span><br><span class="line">    <span class="comment"># 所以v1的滑动平均会被更新为0.1*0+0.9*5 =4.5</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(<span class="string">'更新变量v1=5，滑动平均值'</span>)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新step的值为10000</span></span><br><span class="line">    sess.run(tf.assign(step, <span class="number">10000</span>))</span><br><span class="line">    <span class="comment"># 更新变量v1的值到10</span></span><br><span class="line">    sess.run(tf.assign(v1, <span class="number">10</span>))</span><br><span class="line">    <span class="comment"># 更新v1的滑动平均值。衰减率为 min&#123;0.99 + (1+step)/(10+step)=0.999&#125;=0.99</span></span><br><span class="line">    <span class="comment"># 所以v1的滑动平均会被更新为0.99*4.5+0.01*10 =4.555</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(<span class="string">'更新v1=10,更新step=10000, 滑动平均值'</span>)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 再次更新滑动平均值，得到新的滑动平均值 0.99*4.555+0.01*10=4.60945</span></span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(<span class="string">'再次更新'</span>)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br></pre></td></tr></table></figure>
<pre><code>初始化变量和滑动平均值
[0.0, 0.0]
更新变量v1=5，滑动平均值
[5.0, 4.5]
更新v1=10,更新step=10000, 滑动平均值
[10.0, 4.555]
再次更新
[10.0, 4.60945]
</code></pre>
  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#神经网络优化"><span class="toc-number">1.</span> <span class="toc-text">神经网络优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#学习率设置"><span class="toc-number">1.1.</span> <span class="toc-text">学习率设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#过拟合问题"><span class="toc-number">1.2.</span> <span class="toc-text">过拟合问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Early-stopping"><span class="toc-number">1.2.1.</span> <span class="toc-text">Early stopping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集扩增（Data-augmentation）"><span class="toc-number">1.2.2.</span> <span class="toc-text">数据集扩增（Data augmentation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正则化-（regularization）"><span class="toc-number">1.2.3.</span> <span class="toc-text">正则化 （regularization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-number">1.2.4.</span> <span class="toc-text">Dropout</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#滑动平均模型"><span class="toc-number">1.3.</span> <span class="toc-text">滑动平均模型</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&text=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&is_video=false&description=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Tensorflow 基础（八）： 神经网络优化&body=Check out this article: http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&title=Tensorflow 基础（八）： 神经网络优化"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoururl.com/2018/05/01/Tensorflow 基础（八）：神经网络优化/&name=Tensorflow 基础（八）： 神经网络优化&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menú</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Compartir</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Cap amunt</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 Andy_z
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<!-- clipboard -->

  <script src="/lib/clipboard/clipboard.min.js"></script>
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>

<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
