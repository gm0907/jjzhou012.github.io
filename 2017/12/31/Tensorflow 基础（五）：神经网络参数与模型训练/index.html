<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="神经网络的参数是神经网络实现分类和回归问题的重要部分。下面介绍如何组织、保存以及使用神经网络的参数。 变量初始化Tensorflow 中，变量（tf.Variable）的作用是保存和更新神经网络中的参数。变量需要指定初始值，在神经网络中，给变量赋予随机初始值最为常见，所以一般  使用随机数给变量初始化。 1import tensorflow as tf  用随机数初始化变量：  1weights">
<meta name="keywords" content="Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow 基础（五）：神经网络参数与模型训练">
<meta property="og:url" content="http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/index.html">
<meta property="og:site_name" content="Andy_z &#39;s Blog">
<meta property="og:description" content="神经网络的参数是神经网络实现分类和回归问题的重要部分。下面介绍如何组织、保存以及使用神经网络的参数。 变量初始化Tensorflow 中，变量（tf.Variable）的作用是保存和更新神经网络中的参数。变量需要指定初始值，在神经网络中，给变量赋予随机初始值最为常见，所以一般  使用随机数给变量初始化。 1import tensorflow as tf  用随机数初始化变量：  1weights">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://p5bxip6n0.bkt.clouddn.com/18-4-24/14499218.jpg">
<meta property="og:updated_time" content="2018-04-24T17:28:16.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow 基础（五）：神经网络参数与模型训练">
<meta name="twitter:description" content="神经网络的参数是神经网络实现分类和回归问题的重要部分。下面介绍如何组织、保存以及使用神经网络的参数。 变量初始化Tensorflow 中，变量（tf.Variable）的作用是保存和更新神经网络中的参数。变量需要指定初始值，在神经网络中，给变量赋予随机初始值最为常见，所以一般  使用随机数给变量初始化。 1import tensorflow as tf  用随机数初始化变量：  1weights">
<meta name="twitter:image" content="http://p5bxip6n0.bkt.clouddn.com/18-4-24/14499218.jpg">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Tensorflow 基础（五）：神经网络参数与模型训练</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/01/01/Tensorflow 基础（六）：激活函数 /"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2017/12/31/Tensorflow 基础（四）：常量、序列、随机数/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Post Anterior</span>
      <span id="i-next" class="info" style="display:none;">Post Següent</span>
      <span id="i-top" class="info" style="display:none;">Adalt</span>
      <span id="i-share" class="info" style="display:none;">Compartir Post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&text=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&is_video=false&description=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Tensorflow 基础（五）：神经网络参数与模型训练&body=Check out this article: http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&name=Tensorflow 基础（五）：神经网络参数与模型训练&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#变量初始化"><span class="toc-number">1.</span> <span class="toc-text">变量初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现前向传播"><span class="toc-number">2.</span> <span class="toc-text">实现前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#变量"><span class="toc-number">3.</span> <span class="toc-text">变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#变量类型不可变"><span class="toc-number">3.1.</span> <span class="toc-text">变量类型不可变</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-assign"><span class="toc-number">3.1.1.</span> <span class="toc-text">tf.assign</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#变量维度可变"><span class="toc-number">3.2.</span> <span class="toc-text">变量维度可变</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练神经网络"><span class="toc-number">4.</span> <span class="toc-text">训练神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#完整神经网络样例程序"><span class="toc-number">5.</span> <span class="toc-text">完整神经网络样例程序</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Tensorflow 基础（五）：神经网络参数与模型训练
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Andy_z 's Blog</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2017-12-30T16:01:00.000Z" itemprop="datePublished">2017-12-31</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Tensorflow/">Tensorflow</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Tensorflow/">Tensorflow</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>神经网络的参数是神经网络实现分类和回归问题的重要部分。<br>下面介绍如何组织、保存以及使用神经网络的参数。</p>
<h2 id="变量初始化"><a href="#变量初始化" class="headerlink" title="变量初始化"></a>变量初始化</h2><p>Tensorflow 中，变量（tf.Variable）的作用是保存和更新神经网络中的参数。变量需要指定初始值，在神经网络中，给变量赋予随机初始值最为常见，所以一般  使用随机数给变量初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<ul>
<li>用随机数初始化变量：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p><a href="https://jjzhou012.github.io/2017/12/31/Tensorflow%20%E5%9F%BA%E7%A1%80%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%B8%B8%E9%87%8F%E3%80%81%E5%BA%8F%E5%88%97%E3%80%81%E9%9A%8F%E6%9C%BA%E6%95%B0/" target="_blank" rel="noopener">随机数生成函数</a></p>
<ul>
<li>用常数初始化变量：</li>
</ul>
<p>神经网络中，偏置（bias）通常会使用常数来设置初值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">biases = tf.Variable(tf.zeros([<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<ul>
<li>通过其他变量的初始值来初始化变量：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w2 = tf.Variable(weights.initialized_value())</span><br><span class="line">w3 = tf.Variable(weights.initialized_value() * <span class="number">2.0</span>)</span><br></pre></td></tr></table></figure>
<p>在 Tensorflow 中，一个变量的值在被使用之前，这个变量的初始化过程需要被明确调用。</p>
<h2 id="实现前向传播"><a href="#实现前向传播" class="headerlink" title="实现前向传播"></a>实现前向传播</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明两个变量。通过设置seed保证每次运行结果相同</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入特征向量为常数。 1*2</span></span><br><span class="line">x = tf.constant([[<span class="number">0.7</span>, <span class="number">0.9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过前向传播算法获得神经网络输出</span></span><br><span class="line">a  = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(w1.initializer)            <span class="comment"># 初始化</span></span><br><span class="line">    sess.run(w2.initializer)</span><br><span class="line">    <span class="comment"># 输出</span></span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    <span class="comment"># print(y.eval())</span></span><br></pre></td></tr></table></figure>
<pre><code>[[3.957578]]
</code></pre><p>计算之前，需要将所以用到的变量初始化。<br>当变量数目增多时，或者变量之间存在依赖关系时，单个调用比较麻烦。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全部变量初始化op</span></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
<pre><code>[[3.957578]]
</code></pre><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>类似于张量，维度（shape）和类型（dtype）也是变量的两个重要属性。变量类型不可改变，一个变量在创建之后，类型不能再改变。</p>
<h3 id="变量类型不可变"><a href="#变量类型不可变" class="headerlink" title="变量类型不可变"></a>变量类型不可变</h3><p>例如， 上述 w1 的类型为 random_normal 结果的默认类型 tf.float32, 那么它无法被赋予其他类型的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>), name=<span class="string">'w1'</span>)</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, dtype=tf.float64), name=<span class="string">'w2'</span>)</span><br><span class="line"></span><br><span class="line">w1.assign(w2)     <span class="comment"># tf.assign() 函数用于更新变量的值</span></span><br></pre></td></tr></table></figure>
<pre><code>程序将会报错：  
TypeError: Input &apos;value&apos; of &apos;Assign&apos; Op has type float64 that does not match type float32 of argument &apos;ref&apos;.
</code></pre><h4 id="tf-assign"><a href="#tf-assign" class="headerlink" title="tf.assign"></a>tf.assign</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.assign(  </span><br><span class="line">        ref,                    <span class="comment"># 待赋值的变量。  一个可变的张量。应该来自变量节点。节点可能未初始化。</span></span><br><span class="line">        value,                  <span class="comment"># 更新值。        张量。必须具有与 ref 相同的类型。是要分配给变量的值。</span></span><br><span class="line">        validate_shape=<span class="keyword">None</span>,    <span class="comment"># 一个可选的 bool。默认为 True。如果为 true, 则操作将验证 "value" 的形状是否与分配给的张量的形状相匹配；</span></span><br><span class="line">                                <span class="comment"># 如果为 false, "ref" 将对 "value" 的形状进行引用。 </span></span><br><span class="line">        use_locking=<span class="keyword">None</span>,       <span class="comment"># 一个可选的 bool。默认为 True。如果为 True, 则分配将受锁保护;否则, 该行为是未定义的, 但可能会显示较少的争用。</span></span><br><span class="line">        name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h3 id="变量维度可变"><a href="#变量维度可变" class="headerlink" title="变量维度可变"></a>变量维度可变</h3><p>维度在程序运行时可以改变，需要通过设置参数 <strong>validate_shape=False</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>), name=<span class="string">'w1'</span>)</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">2</span>], stddev=<span class="number">1</span>), name=<span class="string">'w2'</span>)</span><br><span class="line"></span><br><span class="line">tf.assign(w1, w2)</span><br></pre></td></tr></table></figure>
<pre><code>程序报维度不匹配的错误：
ValueError: Dimension 1 in both shapes must be equal, but are 3 and 2. Shapes are [2,3] and [2,2]. for &apos;Assign_1&apos; (op: &apos;Assign&apos;) with input shapes: [2,3], [2,2].
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>), name=<span class="string">'w1'</span>)</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">2</span>], stddev=<span class="number">1</span>), name=<span class="string">'w2'</span>)</span><br><span class="line"></span><br><span class="line">tf.assign(w1, w2, validate_shape=<span class="keyword">False</span>)    <span class="comment"># "ref" 将对 "value" 的形状进行引用。</span></span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor &apos;Assign_2:0&apos; shape=(2, 2) dtype=float32_ref&gt;
</code></pre><h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p><img src="http://p5bxip6n0.bkt.clouddn.com/18-4-24/14499218.jpg" alt=""></p>
<p>反向传播实现了一个迭代过程。每次迭代开始，首先选取一部分训练数据（batch）。这个batch的样例会通过前向传播算法得到神经网络模型的预测结果。可以计算出预测结果与真实结果之间的差距。最后，基于这个差距，反向传播算法会相应更新神经网络的参数取值，使得在这个batch上，预测结果更接近真实结果。</p>
<p>Tensorflow 提供了placeholder机制用于提供输入数据。</p>
<ul>
<li>一个样例的前向传播计算：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>), name=<span class="string">'w1'</span>)</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>), name=<span class="string">'w2'</span>)</span><br><span class="line"><span class="comment"># placeholder 存放输入数据</span></span><br><span class="line">x = tf.placeholder(dtype=tf.float32, shape=(<span class="number">1</span>, <span class="number">2</span>), name=<span class="string">'input'</span>)</span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        sess.run(y)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'需要feed_dict来指定x的取值!'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        print(sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.9</span>]]&#125;))   </span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'!'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>需要feed_dict来指定x的取值!
[[1.650095]]
</code></pre><p>feed_dit 是一个字典（map），在字典中需要给出每个用到的 placeholder 的取值。</p>
<ul>
<li>多batch的前向传播计算：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>), name=<span class="string">'w1'</span>)</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>), name=<span class="string">'w2'</span>)</span><br><span class="line"><span class="comment"># placeholder 存放输入数据</span></span><br><span class="line">x = tf.placeholder(dtype=tf.float32, shape=(<span class="number">3</span>, <span class="number">2</span>), name=<span class="string">'input'</span>)             <span class="comment"># 输入 (1,2) =&gt; (3,2)</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.9</span>], [<span class="number">0.1</span>, <span class="number">0.4</span>],[<span class="number">0.5</span>, <span class="number">0.8</span>]]&#125;))   <span class="comment">#  输入指定n，在运行前向传播时需要提供n个样例数据</span></span><br></pre></td></tr></table></figure>
<pre><code>[[-2.1139326]
 [-0.7940434]
 [-1.7948246]]
</code></pre><p>得到一个 batch 的前行传播结果后， 需要定义一个损失函数来刻画当前预测值和真实值之间的差距。然后通过反向传播算法调整神经网络参数的取值使得差距缩小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义简单的损失函数来刻画预测值和真实值的差距</span></span><br><span class="line">cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, <span class="number">1e-10</span>, <span class="number">1.0</span>)))       <span class="comment"># 交叉熵损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义学习率</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播算法优化神经网络参数</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<h2 id="完整神经网络样例程序"><a href="#完整神经网络样例程序" class="headerlink" title="完整神经网络样例程序"></a>完整神经网络样例程序</h2><p>训练神经网络解决二分类问题：</p>
<ul>
<li>1.定义神经网络结构和前向传播输出结果</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 通过numpy生成模拟数据集</span></span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据batch大小</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 权值参数定义</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在shape的一个维度上使用None可以方便使用不同大小的batch。</span></span><br><span class="line"><span class="comment"># 在训练时需要把数据分成较小的batch，但在测试时可以一次性使用全部数据。</span></span><br><span class="line"><span class="comment"># 当数据集较小时方便测试，数据集较大时，将大量数据放入一个batch可能会导致内存溢出。</span></span><br><span class="line">x = tf.placeholder(dtype=tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>), name=<span class="string">'x-input'</span>)</span><br><span class="line">y_ = tf.placeholder(dtype=tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'y-input'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br></pre></td></tr></table></figure>
<ul>
<li>2.定义损失函数和反向传播优化算法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数和反向传播优化算法</span></span><br><span class="line">cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, <span class="number">1e-10</span>, <span class="number">1.0</span>)))</span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过随机数生成模拟数据集</span></span><br><span class="line">rdm = RandomState(<span class="number">1</span>)        <span class="comment"># 设置seed同时产生随机数</span></span><br><span class="line">dataset_size = <span class="number">128</span></span><br><span class="line">X = rdm.rand(dataset_size, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义规则来给出样本标签。 所有 x1+x2&lt;1 的 样例为正样本，其他为负样本。</span></span><br><span class="line"><span class="comment"># 使用0表示负样本，1表示正样本</span></span><br><span class="line">Y = [[int(x1 + x2 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br></pre></td></tr></table></figure>
<ul>
<li>3.生成会话，在训练数据还上进行反向传播算法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 变量初始化</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(<span class="string">'w1 :'</span> + str(sess.run(w1)))</span><br><span class="line">    print(<span class="string">'w2 :'</span> + str(sess.run(w2)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设定训练轮数</span></span><br><span class="line">    STEPS = <span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        <span class="comment"># 每次选取batch个样本进行训练</span></span><br><span class="line">        start = (i * batch_size) % dataset_size</span><br><span class="line">        end = min(start + batch_size, dataset_size)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 通过选取的样本训练神经网络，更新权值</span></span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x : X[start:end], y_ : Y[start:end]&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 每隔一段时间计算在所有数据上的交叉熵并输出</span></span><br><span class="line">                total_cross_entroy = sess.run(cross_entropy, feed_dict=&#123;x : X, y_ : Y&#125;)</span><br><span class="line">                print(<span class="string">"After %d training step(s), cross entropy in all data is %g"</span> %(i, total_cross_entroy))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练之后的神经网络参数</span></span><br><span class="line">    print(<span class="string">'w1 :'</span> + str(sess.run(w1)))</span><br><span class="line">    print(<span class="string">'w2 :'</span> + str(sess.run(w2)))</span><br></pre></td></tr></table></figure>
<pre><code>w1 :[[-0.8113182   1.4845988   0.06532937]
 [-2.4427042   0.0992484   0.5912243 ]]
w2 :[[-0.8113182 ]
 [ 1.4845988 ]
 [ 0.06532937]]
After 0 training step(s), cross entropy in all data is 0.0674925
After 1000 training step(s), cross entropy in all data is 0.0163385
After 2000 training step(s), cross entropy in all data is 0.00907547
After 3000 training step(s), cross entropy in all data is 0.00714436
After 4000 training step(s), cross entropy in all data is 0.00578471
w1 :[[-1.9618274  2.582354   1.6820377]
 [-3.4681718  1.0698233  2.11789  ]]
w2 :[[-1.8247149]
 [ 2.6854665]
 [ 1.418195 ]]
</code></pre>
  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#变量初始化"><span class="toc-number">1.</span> <span class="toc-text">变量初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现前向传播"><span class="toc-number">2.</span> <span class="toc-text">实现前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#变量"><span class="toc-number">3.</span> <span class="toc-text">变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#变量类型不可变"><span class="toc-number">3.1.</span> <span class="toc-text">变量类型不可变</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-assign"><span class="toc-number">3.1.1.</span> <span class="toc-text">tf.assign</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#变量维度可变"><span class="toc-number">3.2.</span> <span class="toc-text">变量维度可变</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练神经网络"><span class="toc-number">4.</span> <span class="toc-text">训练神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#完整神经网络样例程序"><span class="toc-number">5.</span> <span class="toc-text">完整神经网络样例程序</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&text=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&is_video=false&description=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Tensorflow 基础（五）：神经网络参数与模型训练&body=Check out this article: http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&title=Tensorflow 基础（五）：神经网络参数与模型训练"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoururl.com/2017/12/31/Tensorflow 基础（五）：神经网络参数与模型训练/&name=Tensorflow 基础（五）：神经网络参数与模型训练&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menú</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Compartir</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Cap amunt</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 Andy_z
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Inici</a></li>
         
          <li><a href="/about/">Qui som</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/projects_url">Projectes</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<!-- clipboard -->

  <script src="/lib/clipboard/clipboard.min.js"></script>
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>

<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
